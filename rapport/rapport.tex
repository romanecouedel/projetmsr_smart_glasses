\documentclass[a4paper,12pt]{report}

% Encodage et langue
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}

% Mise en page
\usepackage{geometry}
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=2.5cm,
    bottom=2.5cm
}

% Packages
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{tikz}
\usepackage{float}
\usetikzlibrary{shapes, arrows, positioning}   
\usepackage{everypage} % Pour ajouter le logo sur toutes les pages
\usepackage{longtable}% Pour les tableaux longs

% Fonction pour mettre le logo sur toutes les pages
\newcommand{\AddLogo}{
    \begin{tikzpicture}[remember picture, overlay]
        \node[anchor=north west, xshift=0.75cm, yshift=-0.75cm] 
        at (current page.north west) {\includegraphics[width=3cm]{logo_su.jpg}};
    \end{tikzpicture}
}

% Ajouter le logo à chaque page
\AddEverypageHook{\AddLogo}

\begin{document}

% Numérotation des sections en chiffres  sans le zzero du chapitre
\setcounter{chapter}{0}  
\renewcommand{\thesection}{\arabic{section}}


% Page de garde
\begin{titlepage}
    \centering
    \vspace*{4cm}
    {\Huge \textbf{Smart Glasses}}\\[1.5cm]
    {\Large Authors : Sarah Nacim Evinia Oualid Romane}\\[0.5cm]
    {\Large Master 2 : MSR 2025-2026}\\[3cm]
    \includegraphics[width=0.8\textwidth]{logo.png}% 
\end{titlepage}

% Table des matières
\tableofcontents
\newpage

% Chapitres
\section{Introduction}

\subsection{Project Context}

With the rise of wearable technology, smart glasses are undergoing significant development. Companies such as Meta, with their Ray-Ban Meta glasses, are now offering devices capable of capturing images and video, interacting via voice commands, and accessing AI-based assistants.

At the same time, within the field of assistive technology, several research projects and commercial products aim to support visually impaired or blind individuals specifically through obstacle detection, scene description, and the reading of visual information. Furthermore, recent breakthroughs in artificial intelligence, particularly in natural language processing (NLP) and computer vision, are opening up new possibilities to enhance visual assistance tools.

\subsection{Objectives}

In this context, our project follows a similar path, but with a specific focus on social interaction and assistance. The objective is to design smart glasses capable of detecting both the emotions of the person facing the user and any obstacles within the environment.

However, to ensure reliable operation and straightforward interaction, these two features do not run simultaneously. The system is organized into different operating modes, accessible through an interface of physical buttons integrated into the glasses. This allows the user to switch between "obstacle detection" and "emotion detection" modes, depending on their immediate needs.

This approach simplifies data processing, improves the clarity of haptic feedback, and provides the user with greater control over the device's behavior.

\section{Materials Used}

\begin{longtable}{|p{3cm}|p{2cm}|p{3cm}|p{6cm}|}
\hline
\textbf{Component} & \textbf{Number} & \textbf{Reference / Model} & \textbf{Description / Role} \\
\hline
\endfirsthead

\hline
\textbf{Component} & \textbf{Number} & \textbf{Reference / Model} & \textbf{Description / Role} \\
\hline
\endhead

\hline
\endfoot

\hline
\endlastfoot

Camera & 1 & Camera Raspberry module 2 & Sony IMX219 8-megapixel sensor \\
\hline
Raspberry Pi & 1 & Raspberry Pi 3B+ & Used to process image data and send it to the computer \\
\hline
Arduino & 1 & Arduino Uno & Used to control the haptic feedback system (ie pneumatic actuators...) \\
\hline
Differential Pressure sensor & 2 & DFRobot SEN0343 & Used to detect pressure into balloons \\
\hline
Motor driver & 3 & L298N & Used to control 2 motors per driver \\
\hline
Valves & 2 & DFR0866 & Used to switch between inflate and deflate \\
\hline
Pumps & 4 & DFRobot 370 Mini Vacuum Pump & 2 used to inflate and 2 to deflate balloons \\
\hline
Balloons & 2 & Standard party balloons & Used to do haptic feedback \\
\hline
Push buttons & 2 & Standard push buttons & Used to switch between modes \\
\hline
Computer & 1 & PC linux & Used to run the AI models for emotion and obstacle detection \\
\hline
Battery 12V & 1 & lead acid battery 12V & Used to power the pumps \\
\hline
Battery 5V and <2.5A & 2 & Standard Power bank 5V & Used to power the Raspberry Pi and Arduino \\
\hline

\caption{Main hardware components used in the project}
\end{longtable}


\section{Implementation}
\subsection{Overall Architecture}

\begin{center}
\begin{tikzpicture}[
    block/.style={rectangle, draw, rounded corners, minimum width=3.5cm, minimum height=1cm, align=center},
    arrow/.style={->, thick}
]

% Nodes
\node[block] (user) {User};
\node[block, right=5cm of user] (arduino) {Arduino};
\node[block, above=2cm of user] (raspi) {Raspberry Pi};
\node[block, right=5cm of raspi] (computer) {Computer / AI Models};
% Arrows
\draw[arrow] (raspi.east) -- (computer.west) node[midway, above] {Camera Stream (WIFI)};
\draw[arrow] (computer.south) -- (arduino.north east) node[midway, right] {Control signals};
\draw[arrow] (user.north east) -- (arduino.north west) node[midway, above] {Button press};
\draw[arrow] (arduino.south west) -- (user.south east) node[midway, above] {Haptic feedback};
\draw[arrow] (arduino.north) -- (computer.south west) node[midway, above, sloped] {Mode info};

\end{tikzpicture}
\end{center}

\subsection{Camera and Raspberry pi}
Initially, we attempted to implement a stereo camera system using an Arducam module to obtain depth information directly from two synchronized camera feeds. However, the image quality was insufficient for reliable image processing, which led to poor performance in depth estimation. Moreover, the Raspberry Pi 4B frequently crashed when using the Arducam module, likely due to insufficient processing power and high resource demands of the stereo setup. This instability made the configuration unreliable for real-time operation.


As a result, we switched to using a single camera, combined with AI processing on the computer block to infer depth information from the captured images. This approach allowed us to maintain accurate obstacle and emotion detection while simplifying the hardware setup.

For transmitting the camera stream, we initially experimented with USB gadget mode on a Raspberry Pi 4B. However, this setup proved unstable and presented power supply challenges, as the USB port could not simultaneously provide enough power and transmit data reliably.

To overcome these issues, we opted for a Raspberry Pi 3B+ equipped with Wi-Fi for streaming the camera feed to the computer. Although this solution introduces a small amount of latency, it provides a stable and functional system for real-time image analysis.  

All the configuration steps and installations required to use the Raspberry Pi 3B+ with Wi-Fi for the system are detailed in the file \texttt{procedure\_camera.md}.

\subsection{Pneumatic/haptic system}

\subsection{Communication Arduino - Computer}

Communication between the Arduino and the computer is carried out via a serial port (USB). The Arduino sends mode commands to the PC using the format \texttt{MODE:X}, where X represents the mode selected by the user through the push buttons.

A launcher program (\texttt{launcher.py}) running on the computer continuously listens to the serial port. Upon receiving a mode command, it manages the start and stop of the corresponding Python processes:

\begin{itemize}
    \item \textbf{Mode 1}: Launch of the emotion detection module (\texttt{emotion\_detection2\_pneu.py})
    \item \textbf{Mode 2}: Launch of the obstacle detection module using YOLO (\texttt{objet.py})
    \item \textbf{Mode 3}: Stop of all active scripts (neutral mode)
\end{itemize}

This orchestration system ensures that only one AI module runs at a time, optimizing the use of computational resources and simplifying the delivery of haptic feedback to the user. In the event of an interruption (mode change), the launcher cleanly stops the current process before starting a new one.

\subsection{AI Models and Processing}
The algorithms described below are not executed manually from a terminal, but are launched through the \texttt{launcher.py} script, which manages communication with the Arduino and handles mode switching.  
To check the required installations needed to run the scripts, refer to the \texttt{requirements.txt} file.

\subsubsection{Emotion Detection}

The emotion detection module uses the \textbf{FER} (Facial Expression Recognition) library, which is based on deep neural networks. FER detects seven facial emotions: happy, surprise, sad, angry, fear, disgust, and neutral.\\
To simplify the haptic feedback system and reduce computational load, the seven emotions are grouped into three main categories:

\begin{itemize}
    \item \textbf{Happy}: happy, surprise
    \item \textbf{Sad}: sad, fear, disgust
    \item \textbf{Neutral}: angry, neutral
\end{itemize}

A confidence threshold of 0.35 is applied to ignore low-confidence detections. Each reduced emotion is associated with a confidence score computed from the FER probabilities.

To allow real-time processing, several optimizations are applied:

\begin{itemize}
    \item \textbf{Frame skipping}: only one frame out of three is processed (FRAME\_SKIP = 3)
    \item \textbf{Resizing}: images are scaled to 60\% of their original size
    \item \textbf{Centered detection}: only the face closest to the center of the image is processed\\
\end{itemize}

Each reduced emotion triggers a specific haptic pattern using the pneumatic balloons:

\begin{itemize}
    \item \textbf{Happy}: right balloon inflates and deflates, left balloon inactive
    \item \textbf{Sad}: left balloon inflates and deflates, right balloon inactive
    \item \textbf{Neutral}: both balloons remain inactive
\end{itemize}

The Arduino receives the commands through a serial connection and controls the pneumatic system.

The program receives JPEG-compressed video frames through a UDP socket from the Raspberry Pi. Each frame is decoded and processed by FER at regular intervals. The corresponding haptic commands are then sent to the Arduino. A live display shows the detected face and the recognized emotion.

The emotion is detected only once in order to avoid overstimulating the user.
\subsubsection{Obstacle Detection}

The obstacle detection module is based on the \textbf{YOLO} (You Only Look Once) object detection model combined with the \textbf{Depth Anything V2} network for depth estimation. YOLO is used to detect relevant objects in the scene, while Depth Anything provides a depth map that allows estimating the relative distance of each detected object.

Only a subset of COCO classes is considered, corresponding to potential obstacles for the user (persons, chairs, beds, couches, benches, handbags, suitcases).

The processing pipeline is structured as follows:

\begin{itemize}
    \item \textbf{Frame acquisition}: The program receives JPEG-compressed video frames via a UDP socket from the Raspberry Pi.
 
    \item \textbf{Object detection}: YOLO detects objects and provides bounding boxes and segmentation masks.
    
    \item \textbf{Depth estimation}: Depth Anything computes a dense depth map from the RGB image.
    
    \item \textbf{Depth aggregation}: For each object, the average depth is computed using its segmentation mask.
    
    \item \textbf{Region of Interest (ROI)}: Only objects located inside a trapezoidal region in front of the user are considered.
    
    \item \textbf{Temporal filtering}: An object must be detected continuously for at least one second to be considered stable.
    
    \item \textbf{Side selection}: Only one object per side (left and right) is kept, corresponding to the closest obstacle.
    
    \item \textbf{Intensity computation}: The closest object is assigned an intensity of 100, while the others are scaled proportionally.
    
    \item \textbf{Smoothing}: Haptic intensity variations are limited to avoid sudden pressure changes.
\end{itemize}

The resulting haptic commands are sent to the Arduino, which controls the pneumatic system to provide feedback to the user. The left and right balloons correspond to obstacles detected on the respective sides of the user.

\subsection{User Interface and CAD Design}

\noindent The mechanical design of the smart glasses was developed with a strong focus on ergonomics, stability, and integration of electronic and pneumatic components. The entire structure was modeled using \textbf{SolidWorks}, allowing precise placement of each element and iterative improvements before manufacturing.

\subsubsection{Mechanical Design and 3D Printing}

\noindent The glasses frame was fully designed in SolidWorks and subsequently manufactured using \textbf{3D printing}. The chosen material was \textbf{PLA}, due to its ease of printing, sufficient mechanical rigidity for prototyping, and lightweight properties, which are essential for wearable devices.

\noindent The front part of the glasses integrates both the display and the camera module. The display is positioned in front of the user’s field of view and fixed to the structure using \textbf{M2.5 screws (20 mm)} to ensure mechanical stability. The camera module is mounted securely using \textbf{M2 screws (16 mm)}, allowing accurate alignment with the user’s visual axis.\\

\noindent Figure~\ref{fig:front_glasses} shows the front view of the glasses, highlighting the positioning of the display and the camera.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{front_glasses.png}
    \caption{Front view of the smart glasses showing the display and camera mounting}
    \label{fig:front_glasses}
\end{figure}

\subsubsection{Integration of the Pneumatic Haptic System}

\noindent Inside each branch of the glasses, a rectangular cavity was designed to accommodate the inflatable balloon used for haptic feedback. This cavity allows the balloon to expand and deflate while remaining protected within the structure.

\noindent An additional hole was integrated inside this rectangular cavity to allow the pneumatic tubes to exit toward the exterior side of the glasses. This design ensures clean cable and tube routing, reduces mechanical stress on the tubing, and maintains user comfort.\\

\noindent Figure~\ref{fig:inside_branch} illustrates the internal structure of the glasses branch, including the balloon housing and tube routing.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{inside_branch.png}
    \caption{Internal view of the glasses branch showing the balloon cavity and tube exit}
    \label{fig:inside_branch}
\end{figure}

\subsubsection{Stability and Head Strap Adapter}

\noindent To improve stability during use and ensure proper positioning of the glasses on the user’s head, a head strap adapter was designed and integrated into the frame. This system is inspired by commercial smart glasses such as the Meta smart glasses.

\noindent The head strap significantly enhances comfort and stability, especially during movement, by distributing the weight of the device and preventing unwanted slipping. This design choice is particularly important given the additional weight introduced by the camera, display, and pneumatic system.\\

\noindent Figure~\ref{fig:headstrap} presents the head strap adapter mounted on the glasses.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{headstrap.png}
    \caption{Head strap adapter designed to improve stability and comfort}
    \label{fig:headstrap}
\end{figure}

\subsubsection{User Comfort Considerations}

\noindent Special attention was given to user comfort during prolonged use. A \textbf{nose cut-out} was designed in the front part of the glasses This opening reduces pressure on the nose bridge and improves overall wearability.

\noindent By combining lightweight materials, ergonomic design, and stabilization accessories, the final mechanical design achieves a balance between functionality, comfort, and robustness, making the smart glasses suitable for real-world usage scenarios.


\section{Fonctionnalités}


\section{Objectifs initiaux et limites du projet}

\section{Pistes d'amélioration}

\end{document}
