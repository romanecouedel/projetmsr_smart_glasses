\documentclass[a4paper,12pt]{report}

% Encodage et langue
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}

% Mise en page
\usepackage{geometry}
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=2.5cm,
    bottom=2.5cm
}

% Packages
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{float}
\usetikzlibrary{shapes, arrows, positioning}   
\usepackage{everypage} % Pour ajouter le logo sur toutes les pages
\usepackage{longtable}% Pour les tableaux longs

% Fonction pour mettre le logo sur toutes les pages
\newcommand{\AddLogo}{
    \begin{tikzpicture}[remember picture, overlay]
        \node[anchor=north west, xshift=0.75cm, yshift=-0.75cm] 
        at (current page.north west) {\includegraphics[width=3cm]{logo_su.jpg}};
    \end{tikzpicture}
}

% Ajouter le logo à chaque page
\AddEverypageHook{\AddLogo}

\begin{document}

% Numérotation des sections en chiffres  sans le zzero du chapitre
\setcounter{chapter}{0}  
\renewcommand{\thesection}{\arabic{section}}


% Page de garde
\begin{titlepage}
    \centering
    \vspace*{4cm}
    {\Huge \textbf{Smart Glasses}}\\[1.5cm]
    {\Large Authors : Sarah Nacim Evinia Oualid Romane}\\[0.5cm]
    {\Large Master 2 : MSR 2025-2026}\\[3cm]
    \includegraphics[width=0.8\textwidth]{logo.png}% 
\end{titlepage}

% Table des matières
\tableofcontents
\newpage

% Chapitres
\section{Introduction}

\subsection{Project Context}

With the rise of wearable technology, smart glasses are undergoing significant development. Companies such as Meta, with their Ray-Ban Meta glasses, are now offering devices capable of capturing images and video, interacting via voice commands, and accessing AI-based assistants.

At the same time, within the field of assistive technology, several research projects and commercial products aim to support visually impaired or blind individuals specifically through obstacle detection, scene description, and the reading of visual information. Furthermore, recent breakthroughs in artificial intelligence, particularly in natural language processing (NLP) and computer vision, are opening up new possibilities to enhance visual assistance tools.

\subsection{Objectives}

In this context, our project follows a similar path, but with a specific focus on social interaction and assistance. The objective is to design smart glasses capable of detecting both the emotions of the person facing the user and any obstacles within the environment.

However, to ensure reliable operation and straightforward interaction, these two features do not run simultaneously. The system is organized into different operating modes, accessible through an interface of physical buttons integrated into the glasses. This allows the user to switch between "obstacle detection" and "emotion detection" modes, depending on their immediate needs.

This approach simplifies data processing, improves the clarity of haptic feedback, and provides the user with greater control over the device's behavior.

\section{Materials Used}

\begin{longtable}{|p{3cm}|p{2cm}|p{3cm}|p{6cm}|}
\hline
\textbf{Component} & \textbf{Number} & \textbf{Reference / Model} & \textbf{Description / Role} \\
\hline
\endfirsthead

\hline
\textbf{Component} & \textbf{Number} & \textbf{Reference / Model} & \textbf{Description / Role} \\
\hline
\endhead

\hline
\endfoot

\hline
\endlastfoot

Camera & 1 & Camera Raspberry module 2 & Sony IMX219 8-megapixel sensor \\
\hline
Raspberry Pi & 1 & Raspberry Pi 3B+ & Used to process image data and send it to the computer \\
\hline
Arduino & 1 & Arduino Uno & Used to control the haptic feedback system (ie pneumatic actuators...) \\
\hline
Differential Pressure sensor & 2 & DFRobot SEN0343 & Used to detect pressure into balloons \\
\hline
Motor driver & 3 & L298N & Used to control 2 motors per driver \\
\hline
Valves & 2 & DFR0866 & Used to switch between inflate and deflate \\
\hline
Pumps & 4 & DFRobot 370 Mini Vacuum Pump & 2 used to inflate and 2 to deflate balloons \\
\hline
Balloons & 2 & Standard party balloons & Used to do haptic feedback \\
\hline
Push buttons & 2 & Standard push buttons & Used to switch between modes \\
\hline
Computer & 1 & PC linux & Used to run the AI models for emotion and obstacle detection \\
\hline
Battery 12V & 1 & lead acid battery 12V & Used to power the pumps \\
\hline
Battery 5V and <2.5A & 2 & Standard Power bank 5V & Used to power the Raspberry Pi and Arduino \\
\hline

\caption{Main hardware components used in the project}
\end{longtable}


\section{Implementation}
\subsection{Overall Architecture}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{archi.png}
\caption{Overall system architecture of the smart glasses. The system comprises four main components: (1) the user wearing the glasses with integrated balloons controlled by a pneumatics system (2) the Arduino microcontroller managing the pneumatic feedback system, (3) the Raspberry Pi capturing and streaming video frames via Wi-Fi, and (4) the host computer running AI models for emotion and obstacle detection. Communication flows bidirectionally: the camera stream is transmitted from the Raspberry Pi to the computer, while control signals and pressure commands are exchanged between the computer and Arduino via serial communication. User mode selection is handled through physical buttons connected to the Arduino.}
\label{fig:architecture}
\end{figure}


\subsection{Camera and Raspberry pi}
Initially, we attempted to implement a stereo camera system using an Arducam module to obtain depth information directly from two synchronized camera feeds. However, the image quality was insufficient for reliable image processing, which led to poor performance in depth estimation. Moreover, the Raspberry Pi 4B frequently crashed when using the Arducam module, likely due to insufficient processing power and high resource demands of the stereo setup. This instability made the configuration unreliable for real-time operation.


As a result, we switched to using a single camera, combined with AI processing on the computer block to infer depth information from the captured images. This approach allowed us to maintain accurate obstacle and emotion detection while simplifying the hardware setup.

For transmitting the camera stream, we initially experimented with USB gadget mode on a Raspberry Pi 4B. However, this setup proved unstable and presented power supply challenges, as the USB port could not simultaneously provide enough power and transmit data reliably.

To overcome these issues, we opted for a Raspberry Pi 3B+ equipped with Wi-Fi for streaming the camera feed to the computer. Although this solution introduces a small amount of latency, it provides a stable and functional system for real-time image analysis.  

All the configuration steps and installations required to use the Raspberry Pi 3B+ with Wi-Fi for the system are detailed in the file \texttt{procedure\_camera.md}.

\subsection{Pneumatic/haptic system}

The haptic feedback mechanism employed in this project draws inspiration from the OpenPneu platform developed by Tian et al. \cite{openpneu2024}, which presents a compact architecture for multi-channel pneumatic actuation. This approach was selected for its ability to provide precise, controllable haptic sensations through pressure modulation in soft actuators.

\subsubsection{System Architecture}

The pneumatic system comprises four principal components per haptic channel: (i) an inflatable balloon serving as the soft actuator, (ii) a differential pressure sensor (DFRobot SEN0343) for real-time pressure monitoring, (iii) a 3/2-way solenoid valve (DFR0866) for airflow direction control, and (iv) a dual-pump configuration consisting of one inflation pump and one deflation pump (DFRobot 370 Mini Vacuum Pump). The complete system architecture is illustrated in Figure~\ref{fig:pneumatics}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{pneumatics.png}
\caption{Schematic diagram of the pneumatic actuation system for a single haptic channel, adapted from OpenPneu platform \cite{openpneu2024}. The system integrates a 3/2-way valve, differential pressure sensor, inflation and deflation pumps, and soft balloon actuator.}
\label{fig:pneumatics}
\end{figure}

\subsubsection{Valve Configuration and Operation}

The 3/2-way solenoid valve features three ports and two stable positions, enabling bidirectional airflow control. In its default state, one port remains permanently open while the valve alternately connects either the inflation or deflation pathway. When the valve switches position, the previously open port closes while the opposite port opens, thereby reversing the airflow direction. This configuration allows for independent control of inflation and deflation processes without requiring additional valves.

\subsubsection{Pressure Control Algorithm}

The system implements a closed-loop pressure control scheme based on proportional feedback. The pressure setpoint $P_{\text{target}}$ is continuously compared with the measured pressure $P_{\text{measured}}$ from the differential sensor to compute the control error:

\begin{equation}
e(t) = P_{\text{target}} - P_{\text{measured}}
\end{equation}

The control logic operates as follows:
\begin{itemize}
    \item If $e(t) > 0$ (positive error), the valve switches to the inflation configuration, activating the inflation pump until the measured pressure reaches the target value.
    \item If $e(t) < 0$ (negative error), the valve switches to the deflation configuration, activating the deflation pump to reduce pressure until equilibrium is achieved.
    \item When $|e(t)|$ falls below a predefined threshold, both pumps are deactivated to prevent oscillations and reduce power consumption.
\end{itemize}

This feedback control approach ensures rapid and precise pressure regulation, enabling the generation of distinct haptic patterns for different feedback modalities (obstacle detection and emotion recognition). The dual-pump architecture permits simultaneous inflation and deflation capabilities, significantly improving response time compared to passive deflation systems.

\subsubsection{Dual-Channel Implementation}

To provide bilateral haptic feedback corresponding to the left and right sides of the glasses frame, the entire pneumatic system described above is duplicated, resulting in two independent actuation channels. Each channel operates autonomously with its own balloon actuator, pressure sensor, valve, and pump pair. The Arduino microcontroller manages both channels simultaneously, receiving target pressure commands from the host computer via serial communication.

\subsubsection{I2C Address Conflict and Sensor Simulation}

During implementation, a significant hardware limitation was encountered related to the pressure sensors. Both DFRobot SEN0343 differential pressure sensors communicate via the I2C protocol and are factory-configured with identical, non-configurable I2C addresses. Since the Arduino Uno possesses only a single I2C bus, it is unable to distinguish between the two sensors, preventing simultaneous operation of both channels with hardware pressure feedback.

To circumvent this constraint, a software-based sensor simulation approach was developed. The functional pressure sensor was first used to empirically characterize the system's pressure dynamics under three operational conditions: (i) active inflation, (ii) active deflation, and (iii) idle state (no pump activity). Through systematic experimentation, the pressure variation rates were quantified for each condition.

Based on these empirical measurements, a virtual pressure variable \texttt{pressure\_simulated} was implemented in the Arduino firmware. This variable is incremented or decremented at each control cycle according to the current valve state and pump activity, effectively modeling the expected pressure evolution. The control algorithm then operates using this simulated pressure value rather than direct sensor readings.

While this approach successfully enabled dual-channel operation, it presents inherent limitations. The simulated sensor cannot detect external disturbances such as physical contact with the balloon, which would cause unexpected pressure variations. A genuine pressure sensor would immediately detect such anomalies and adjust the control accordingly, whereas the simulation continues to assume nominal behavior. Despite this limitation, the system performance proved satisfactory for the intended application, as the glasses are designed to be worn in a relatively stable configuration where external interference with the actuators is minimal.




\subsection{Communication Arduino - Computer}

Communication between the Arduino and the computer is carried out via a serial port (USB). The Arduino sends mode commands to the PC using the format \texttt{MODE:X}, where X represents the mode selected by the user through the push buttons.

A launcher program (\texttt{launcher.py}) running on the computer continuously listens to the serial port. Upon receiving a mode command, it manages the start and stop of the corresponding Python processes:

\begin{itemize}
    \item \textbf{Mode 1}: Launch of the emotion detection module (\texttt{emotion\_detection2\_pneu.py})
    \item \textbf{Mode 2}: Launch of the obstacle detection module using YOLO (\texttt{objet.py})
    \item \textbf{Mode 3}: Stop of all active scripts (neutral mode)
\end{itemize}

This orchestration system ensures that only one AI module runs at a time, optimizing the use of computational resources and simplifying the delivery of haptic feedback to the user. In the event of an interruption (mode change), the launcher cleanly stops the current process before starting a new one.

\subsection{AI Models and Processing}
The algorithms described below are not executed manually from a terminal, but are launched through the \texttt{launcher.py} script, which manages communication with the Arduino and handles mode switching.  
To check the required installations needed to run the scripts, refer to the \texttt{requirements.txt} file.

\subsubsection{Emotion Detection}

The emotion detection module uses the \textbf{FER} (Facial Expression Recognition) library, which is based on deep neural networks. FER detects seven facial emotions: happy, surprise, sad, angry, fear, disgust, and neutral.\\
To simplify the haptic feedback system and reduce computational load, the seven emotions are grouped into three main categories:

\begin{itemize}
    \item \textbf{Happy}: happy, surprise
    \item \textbf{Sad}: sad, fear, disgust
    \item \textbf{Neutral}: angry, neutral
\end{itemize}

A confidence threshold of 0.35 is applied to ignore low-confidence detections. Each reduced emotion is associated with a confidence score computed from the FER probabilities.

To allow real-time processing, several optimizations are applied:

\begin{itemize}
    \item \textbf{Frame skipping}: only one frame out of three is processed (FRAME\_SKIP = 3)
    \item \textbf{Resizing}: images are scaled to 60\% of their original size
    \item \textbf{Centered detection}: only the face closest to the center of the image is processed\\
\end{itemize}

Each reduced emotion triggers a specific haptic pattern using the pneumatic balloons:

\begin{itemize}
    \item \textbf{Happy}: right balloon inflates and deflates, left balloon inactive
    \item \textbf{Sad}: left balloon inflates and deflates, right balloon inactive
    \item \textbf{Neutral}: both balloons remain inactive
\end{itemize}

The Arduino receives the commands through a serial connection and controls the pneumatic system.

The program receives JPEG-compressed video frames through a UDP socket from the Raspberry Pi. Each frame is decoded and processed by FER at regular intervals. The corresponding haptic commands are then sent to the Arduino. A live display shows the detected face and the recognized emotion.

The emotion is detected only once in order to avoid overstimulating the user.
\subsubsection{Obstacle Detection}

The obstacle detection module is based on the \textbf{YOLO} (You Only Look Once) object detection model combined with the \textbf{Depth Anything V2} network for depth estimation. YOLO is used to detect relevant objects in the scene, while Depth Anything provides a depth map that allows estimating the relative distance of each detected object.

Only a subset of COCO classes is considered, corresponding to potential obstacles for the user (persons, chairs, beds, couches, benches, handbags, suitcases).

The processing pipeline is structured as follows:

\begin{itemize}
    \item \textbf{Frame acquisition}: The program receives JPEG-compressed video frames via a UDP socket from the Raspberry Pi.
 
    \item \textbf{Object detection}: YOLO detects objects and provides bounding boxes for localization. To improve spatial accuracy, we employ the YOLOv11-seg variant, which extends the base YOLO architecture with instance segmentation capabilities. Unlike standard object detection that only provides rectangular bounding boxes, YOLO-seg generates pixel-precise segmentation masks for each detected object (see Figure~\ref{fig:yolo_detection}). These masks identify all pixels belonging to the object, providing a significantly more accurate representation of the object's spatial extent compared to the entire bounding box region.
    
    \item \textbf{Depth estimation}: Depth Anything computes a dense depth map from the RGB image (Figure~\ref{fig:depth_map}).
    
    \item \textbf{Depth aggregation}: For each detected object, the average depth is computed by combining information from the depth map and the segmentation mask. The depth map provides a depth value for every pixel in the image, while the segmentation mask identifies which pixels belong to the detected object. By indexing the depth map with the mask coordinates, we extract the depth values corresponding exclusively to pixels within the object. These depth values are then averaged to obtain a single representative depth estimate for the object.

    
    \item \textbf{Region of Interest (ROI)}: Only objects located inside a trapezoidal region in front of the user are considered (illustrated in Figure~\ref{fig:roi}). This trapezoidal shape is specifically designed to respect the natural perspective of human vision: wide at the bottom (near field) and narrow at the top (far field). The geometry of this ROI closely approximates the user's walking path and immediate surroundings, focusing detection efforts on the zone where obstacles pose an actual risk during navigation. Objects detected outside this region, even if visible in the camera frame, are filtered out as they are sufficiently distant or laterally displaced to be irrelevant for immediate obstacle avoidance. This spatial filtering reduces computational load and prevents unnecessary haptic alerts for obstacles that the user is unlikely to encounter.

    
    \item \textbf{Temporal filtering}: An object must be detected continuously for at least one second to be considered stable.
    
    \item \textbf{Intensity and side selection}: Only one object per side (left and right) is kept, corresponding to the closest obstacle on each respective side. When obstacles are detected on both sides, the haptic feedback intensity is computed using a proportional relationship based on relative distances. The closest object, regardless of which side it appears on, is always assigned a maximum haptic intensity of 100\%. The intensity for the more distant object is then scaled proportionally according to the depth difference between the two obstacles.

    
   
\end{itemize}

\FloatBarrier

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{yolo_exemple.png}
\caption{Example of YOLOv11-seg object detection. The image shows detected obstacles with bounding boxes (rectangles) and pixel-precise segmentation masks (colored overlays).}
\label{fig:yolo_detection}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{depth_ex.png}
\caption{Dense depth map generated by Depth Anything V2 from an RGB input frame. Warmer regions correspond to closer objects, while colder regions indicate more distant areas.}
\label{fig:depth_map}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{roi_ex.png}
\caption{Trapezoidal Region of Interest (ROI) overlaid on the camera view. The trapezoid is wide at the bottom (near field) and narrow at the top (far field), respecting natural perspective. Only obstacles detected within this region trigger haptic feedback, focusing attention on the user's immediate walking path.}
\label{fig:roi}
\end{figure}

\FloatBarrier

The resulting haptic commands are sent to the Arduino, which controls the pneumatic system to provide feedback to the user. The left and right balloons correspond to obstacles detected on the respective sides of the user.

\subsection{User Interface and CAD Design}

\noindent The mechanical design of the smart glasses was developed with a strong focus on ergonomics, stability, and integration of electronic and pneumatic components. The entire structure was modeled using \textbf{SolidWorks}, allowing precise placement of each element and iterative improvements before manufacturing.

\subsubsection{Mechanical Design and 3D Printing}

\noindent The glasses frame was fully designed in SolidWorks and subsequently manufactured using \textbf{3D printing}. The chosen material was \textbf{PLA}, due to its ease of printing, sufficient mechanical rigidity for prototyping, and lightweight properties, which are essential for wearable devices.

\noindent The front part of the glasses integrates both the display and the camera module. The display is positioned in front of the user’s field of view and fixed to the structure using \textbf{M2.5 screws (20 mm)} to ensure mechanical stability. The camera module is mounted securely using \textbf{M2 screws (16 mm)}, allowing accurate alignment with the user’s visual axis.\\

\noindent Figure~\ref{fig:front_glasses} shows the front view of the glasses, highlighting the positioning of the display and the camera.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{front_glasses.png}
    \caption{Front view of the smart glasses showing the display and camera mounting}
    \label{fig:front_glasses}
\end{figure}

\subsubsection{Integration of the Pneumatic Haptic System}

\noindent Inside each branch of the glasses, a rectangular cavity was designed to accommodate the inflatable balloon used for haptic feedback. This cavity allows the balloon to expand and deflate while remaining protected within the structure.

\noindent An additional hole was integrated inside this rectangular cavity to allow the pneumatic tubes to exit toward the exterior side of the glasses. This design ensures clean cable and tube routing, reduces mechanical stress on the tubing, and maintains user comfort.\\

\noindent Figure~\ref{fig:inside_branch} illustrates the internal structure of the glasses branch, including the balloon housing and tube routing.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{inside_branch.png}
    \caption{Internal view of the glasses branch showing the balloon cavity and tube exit}
    \label{fig:inside_branch}
\end{figure}

\subsubsection{Stability and Head Strap Adapter}

\noindent To improve stability during use and ensure proper positioning of the glasses on the user’s head, a head strap adapter was designed and integrated into the frame. This system is inspired by commercial smart glasses such as the Meta smart glasses.

\noindent The head strap significantly enhances comfort and stability, especially during movement, by distributing the weight of the device and preventing unwanted slipping. This design choice is particularly important given the additional weight introduced by the camera, display, and pneumatic system.\\

\noindent Figure~\ref{fig:headstrap} presents the head strap adapter mounted on the glasses.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{headstrap.png}
    \caption{Head strap adapter designed to improve stability and comfort}
    \label{fig:headstrap}
\end{figure}

\subsubsection{User Comfort Considerations}

\noindent Special attention was given to user comfort during prolonged use. A \textbf{nose cut-out} was designed in the front part of the glasses This opening reduces pressure on the nose bridge and improves overall wearability.

\noindent By combining lightweight materials, ergonomic design, and stabilization accessories, the final mechanical design achieves a balance between functionality, comfort, and robustness, making the smart glasses suitable for real-world usage scenarios.


\section{Fonctionnalités}


\section{Objectifs initiaux et limites du projet}

\section{Pistes d'amélioration}

\begin{thebibliography}{9}
\bibitem{openpneu2024}
Y. Tian, R. Su, X. Wang, N. B. Altin, G. Fang, and C. C. L. Wang,
``OpenPneu: Compact Platform for Pneumatic Actuation with Multi-Channels,''
[Conference/Journal details], 2024.
\end{thebibliography}
\end{document}
