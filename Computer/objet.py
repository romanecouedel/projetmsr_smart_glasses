from ultralytics import YOLO
import cv2
import torch
import numpy as np
import serial
import time
from depth_anything_v2.depth_anything_v2.dpt import DepthAnythingV2


import socket
import struct






# ========== Configuration Serial (Arduino) ==========
ARDUINO_PORT = "/dev/ttyACM1"
ARDUINO_BAUDRATE = 9600
arduino = None

# ========== Classes d'objets √† d√©tecter ==========
OBJECT_CLASSES = [0, 13, 28, 56, 59, 26, 57]

# Noms des classes COCO
COCO_NAMES = {
    0: 'person', 13: 'bench', 28:'suitcase', 57:'couch',
    56: 'chair', 59: 'bed', 26: 'handbag'
}

# ========== Syst√®me de filtrage temporel ==========
class ObjectTracker:
    """Suit les objets d√©tect√©s au fil du temps pour √©viter les changements brusques"""
    
    def __init__(self, stability_duration=1.0, intensity_step=15):
        """
        stability_duration: dur√©e minimale (secondes) qu'un objet doit √™tre d√©tect√©
        intensity_step: changement max d'intensit√© par frame (pour lisser)
        """
        self.tracked_objects = {}  # {side: {'class': int, 'first_seen': time, 'last_seen': time, 'intensity': int}}
        self.stability_duration = stability_duration
        self.intensity_step = intensity_step
        self.last_intensities = {'left': 0, 'right': 0}
    
    def update(self, side, obj_class, intensity, current_time):
        """
        Met √† jour le tracking d'un objet
        Retourne l'intensit√© liss√©e si l'objet est stable, None sinon
        """
        key = side
        
        # Nouvel objet ou objet diff√©rent d√©tect√©
        if key not in self.tracked_objects or self.tracked_objects[key]['class'] != obj_class:
            self.tracked_objects[key] = {
                'class': obj_class,
                'first_seen': current_time,
                'last_seen': current_time,
                'intensity': intensity
            }
            return None  # Pas encore stable
        
        # M√™me objet d√©tect√©, mettre √† jour
        self.tracked_objects[key]['last_seen'] = current_time
        
        # V√©rifier si l'objet est stable (d√©tect√© depuis assez longtemps)
        time_stable = current_time - self.tracked_objects[key]['first_seen']
        if time_stable < self.stability_duration:
            return None  # Pas encore stable
        
        # Objet stable, lisser l'intensit√©
        last_intensity = self.last_intensities[side]
        smoothed_intensity = self._smooth_intensity(last_intensity, intensity)
        self.last_intensities[side] = smoothed_intensity
        
        return smoothed_intensity
    
    def _smooth_intensity(self, current, target):
        """Lisse le changement d'intensit√© avec un step maximum"""
        diff = target - current
        
        if abs(diff) <= self.intensity_step:
            return target
        elif diff > 0:
            return current + self.intensity_step
        else:
            return current - self.intensity_step
    
    def clear_side(self, side, current_time):
        """Nettoie un c√¥t√© si aucun objet n'est d√©tect√©"""
        if side in self.tracked_objects:
            # Garder pendant un court moment pour √©viter les flickerings
            if current_time - self.tracked_objects[side]['last_seen'] > 0.5:
                del self.tracked_objects[side]
                self.last_intensities[side] = 0
    
    def reset(self):
        """R√©initialise compl√®tement le tracker"""
        self.tracked_objects = {}
        self.last_intensities = {'left': 0, 'right': 0}

# ========== Initialisation Arduino ==========
def init_arduino():
    global arduino
    try:
        arduino = serial.Serial(ARDUINO_PORT, ARDUINO_BAUDRATE, timeout=1)
        time.sleep(2)
        print("‚úì Connexion Arduino √©tablie")
        return True
    except Exception as e:
        print(f"‚úó Erreur connexion Arduino: {e}")
        print("   V√©rifiez le port et les droits d'acc√®s")
        return False

def send_command(command):
    """Envoie une commande √† l'Arduino"""
    if arduino and arduino.is_open:
        try:
            arduino.write(f"{command}\n".encode())
            print(f"üì§ Arduino: {command}")
        except Exception as e:
            print(f"‚úó Erreur envoi: {e}")

# ========== Configuration Depth Anything V2 ==========
DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'

model_configs = {
    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},
    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}
}

encoder = 'vitb'

print(f"üîß Chargement du mod√®le Depth Anything V2 ({encoder}) sur {DEVICE}...")
depth_model = DepthAnythingV2(**model_configs[encoder])
depth_model.load_state_dict(torch.load(f'depth_anything_v2/checkpoints/depth_anything_v2_{encoder}.pth', map_location='cpu'))
depth_model = depth_model.to(DEVICE).eval()
print("‚úì Mod√®le Depth charg√©\n")

# ========== Chargement YOLO ==========
print("üîß Chargement du mod√®le YOLO...")
yolo_model = YOLO("yolo11x-seg.pt")
print("‚úì Mod√®le YOLO charg√©\n")

# ========== Fonctions Depth ==========
def visualize_depth(depth_map):
    """Convertit la carte de profondeur en image color√©e"""
    depth_normalized = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())
    depth_normalized = 1 - depth_normalized
    depth_colored = cv2.applyColorMap((depth_normalized * 255).astype(np.uint8), cv2.COLORMAP_TURBO)
    return depth_colored

def calculate_mask_depth(mask, depth_map):
    """
    Calcule la profondeur moyenne d'un objet √† partir de son masque
    mask: masque binaire de l'objet (numpy array)
    depth_map: carte de profondeur (numpy array)
    Returns: profondeur moyenne normalis√©e (0-1, 0=proche, 1=loin)
    """
    # Redimensionner le masque √† la taille de la depth map si n√©cessaire
    if mask.shape[:2] != depth_map.shape[:2]:
        mask = cv2.resize(mask.astype(np.uint8), (depth_map.shape[1], depth_map.shape[0]))
    
    # Extraire les valeurs de profondeur l√† o√π le masque est actif
    mask_bool = mask > 0.5
    if not mask_bool.any():
        return None
    
    depth_values = depth_map[mask_bool]
    avg_depth = np.mean(depth_values)
    
    # Normaliser par rapport √† min/max de toute la carte
    normalized_depth = (avg_depth - depth_map.min()) / (depth_map.max() - depth_map.min())
    
    return normalized_depth

def depth_to_intensity(normalized_depth, closest_depth):
    """
    Convertit une profondeur normalis√©e en intensit√© (0-100)
    L'objet le plus proche est toujours √† 100%
    Les autres sont calcul√©s proportionnellement
    
    IMPORTANT: Dans depth map, valeurs √âLEV√âES = PROCHE, valeurs BASSES = LOIN
    normalized_depth: profondeur de l'objet actuel (1=proche, 0=loin)
    closest_depth: profondeur de l'objet le plus proche globalement (1=proche, 0=loin)
    """
    # V√©rifier si c'est l'objet le plus proche (ou tr√®s proche)
    if normalized_depth >= closest_depth * 0.99:  # Tol√©rance de 1%
        return 100
    
    # Calcul proportionnel: plus loin = moins intense
    # Si closest = 0.8 et current = 0.4, alors ratio = 0.4/0.8 = 0.5 -> 50%
    if closest_depth > 0:
        intensity = int((normalized_depth / closest_depth) * 100)
    else:
        intensity = 10
    
    return max(10, min(100, intensity))  # Limiter entre 10 et 100

# ========== Fonctions ROI ==========
def is_in_roi(box_center_x, box_center_y, frame_width, frame_height):
    """V√©rifie si le centre de la bounding box est dans la zone d'int√©r√™t"""
    roi_top = 0
    roi_bottom = frame_height
    
    if box_center_y < roi_top:
        return False
    
    progress = (box_center_y - roi_top) / (roi_bottom - roi_top)
    roi_width_ratio = 0.2 + (0.6 * progress)
    
    roi_left = frame_width * (1 - roi_width_ratio) / 2
    roi_right = frame_width * (1 + roi_width_ratio) / 2
    
    return roi_left <= box_center_x <= roi_right

def draw_roi(frame, frame_width, frame_height):
    """Dessine la zone d'int√©r√™t en forme de trap√®ze"""
    roi_top = 0
    roi_bottom = frame_height
    
    top_width = int(frame_width * 0.2)
    bottom_width = int(frame_width * 0.8)
    pts = np.array([
        [frame_width // 2 - top_width // 2, roi_top],
        [frame_width // 2 + top_width // 2, roi_top],
        [frame_width // 2 + bottom_width // 2, roi_bottom],
        [frame_width // 2 - bottom_width // 2, roi_bottom]
    ], np.int32)
    
    overlay = frame.copy()
    cv2.fillPoly(overlay, [pts], (0, 255, 0, 30))
    cv2.addWeighted(overlay, 0.3, frame, 0.7, 0, frame)
    cv2.polylines(frame, [pts], True, (0, 255, 0), 2)
    
    return frame

def process_detection_with_depth(objects_data, frame_width, frame_height, tracker, current_time):
    """
    Analyse les objets avec leur profondeur et contr√¥le les ballons
    objects_data: liste de dict avec 'box', 'mask', 'depth', 'class', 'center'
    tracker: ObjectTracker pour le filtrage temporel
    current_time: timestamp actuel
    
    CONTRAINTES:
    - L'objet le plus proche globalement est toujours √† intensit√© 100%
    - Les autres objets ont une intensit√© proportionnelle √† leur distance
    - On ne garde qu'UN SEUL objet par c√¥t√© (le plus proche)
    - Filtrage: objet doit √™tre d√©tect√© pendant 1s minimum
    - Lissage: changement d'intensit√© progressif
    """
    center_x = frame_width / 2
    left_objects = []
    right_objects = []
    
    # Filtrer les objets dans la ROI
    roi_objects = []
    for obj in objects_data:
        box_center_x, box_center_y = obj['center']
        if is_in_roi(box_center_x, box_center_y, frame_width, frame_height):
            roi_objects.append(obj)
            if box_center_x < center_x:
                left_objects.append(obj)
            else:
                right_objects.append(obj)
    
    if len(roi_objects) == 0:
        # Nettoyer les c√¥t√©s et arr√™ter progressivement
        tracker.clear_side('left', current_time)
        tracker.clear_side('right', current_time)
        
        # Envoyer STOP seulement si les deux intensit√©s sont √† 0
        if tracker.last_intensities['left'] == 0 and tracker.last_intensities['right'] == 0:
            send_command("STOP")
            print("\n‚úì Aucun obstacle dans la ROI")
        return
    
    # Trouver l'objet le plus proche globalement (profondeur MAXIMALE car proche = valeur √©lev√©e)
    closest_global = max(roi_objects, key=lambda x: x['depth'])
    closest_depth = closest_global['depth']
    
    # Recalculer les intensit√©s en fonction de l'objet le plus proche
    for obj in roi_objects:
        obj['intensity'] = depth_to_intensity(obj['depth'], closest_depth)
    
    print("\n" + "="*60)
    print("üéØ OBJETS D√âTECT√âS DANS LA ROI:")
    print("="*60)
    
    # CONTRAINTE: Garder UN SEUL objet par c√¥t√© (le plus proche = depth max)
    selected_left = None
    selected_right = None
    final_left_intensity = None
    final_right_intensity = None
    
    if len(left_objects) > 0:
        selected_left = max(left_objects, key=lambda x: x['depth'])
        
        # Appliquer le filtrage temporel
        smoothed_intensity = tracker.update('left', selected_left['class'], 
                                           selected_left['intensity'], current_time)
        
        if smoothed_intensity is not None:
            final_left_intensity = smoothed_intensity
            side = "GAUCHE"
            class_name = COCO_NAMES.get(selected_left['class'], f"class_{selected_left['class']}")
            depth_percent = selected_left['depth'] * 100
            status = "‚úì STABLE" if smoothed_intensity == selected_left['intensity'] else "‚è≥ LISSAGE"
            print(f"  [{side:6}] {class_name:10} | D:{depth_percent:5.1f}% | I:{smoothed_intensity:3}% | {status}")
        else:
            # Objet pas encore stable
            class_name = COCO_NAMES.get(selected_left['class'], f"class_{selected_left['class']}")
            print(f"  [GAUCHE] {class_name:10} | ‚è±Ô∏è  EN ATTENTE (stabilisation...)")
    else:
        tracker.clear_side('left', current_time)
    
    if len(right_objects) > 0:
        selected_right = max(right_objects, key=lambda x: x['depth'])
        
        # Appliquer le filtrage temporel
        smoothed_intensity = tracker.update('right', selected_right['class'], 
                                           selected_right['intensity'], current_time)
        
        if smoothed_intensity is not None:
            final_right_intensity = smoothed_intensity
            side = "DROITE"
            class_name = COCO_NAMES.get(selected_right['class'], f"class_{selected_right['class']}")
            depth_percent = selected_right['depth'] * 100
            status = "‚úì STABLE" if smoothed_intensity == selected_right['intensity'] else "‚è≥ LISSAGE"
            print(f"  [{side:6}] {class_name:10} | D:{depth_percent:5.1f}% | I:{smoothed_intensity:3}% | {status}")
        else:
            # Objet pas encore stable
            class_name = COCO_NAMES.get(selected_right['class'], f"class_{selected_right['class']}")
            print(f"  [DROITE] {class_name:10} | ‚è±Ô∏è  EN ATTENTE (stabilisation...)")
    else:
        tracker.clear_side('right', current_time)
    
    # Contr√¥ler les ballons SEULEMENT avec les objets stables
    if final_left_intensity is not None and final_right_intensity is not None:
        send_command(f"GG,{final_left_intensity+30}")
        time.sleep(0.3)
        send_command(f"GD,{final_right_intensity}")
        time.sleep(0.3)
        print(f"\n‚ö†Ô∏è  ALERTE: Obstacles des DEUX c√¥t√©s")
        
    elif final_left_intensity is not None:
        send_command(f"GG,{final_left_intensity}")
        time.sleep(0.3)
        # R√©duire progressivement le c√¥t√© droit si n√©cessaire
        if tracker.last_intensities['right'] > 0:
            new_right = max(0, tracker.last_intensities['right'] - tracker.intensity_step)
            if new_right > 0:
                send_command(f"GD,{new_right}")
                time.sleep(0.3)
                tracker.last_intensities['right'] = new_right
        print(f"\n‚ö†Ô∏è  ALERTE: Obstacle √† GAUCHE")
        
    elif final_right_intensity is not None:
        send_command(f"GD,{final_right_intensity}")
        time.sleep(0.3)
        # R√©duire progressivement le c√¥t√© gauche si n√©cessaire
        if tracker.last_intensities['left'] > 0:
            new_left = max(0, tracker.last_intensities['left'] - tracker.intensity_step)
            if new_left > 0:
                
                send_command(f"GG,{new_left}")
                time.sleep(0.3)
                tracker.last_intensities['left'] = new_left
        print(f"\n‚ö†Ô∏è  ALERTE: Obstacle √† DROITE")
    
    print("="*60)

# ========== Boucle Principale ==========
def main():
    # Ouvrir la cam√©ra
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("‚úó Erreur: Cam√©ra non trouv√©e")
        return
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"‚úì Cam√©ra activ√©e ({frame_width}x{frame_height}px)")
    print("üìπ Appuyez sur 'q' pour quitter\n")

    tracker = ObjectTracker(stability_duration=1.0, intensity_step=15)
    
    # Cr√©er les 4 fen√™tres
    cv2.namedWindow("1. Camera RGB", cv2.WINDOW_NORMAL)
    cv2.namedWindow("2. Depth Estimation", cv2.WINDOW_NORMAL)
    cv2.namedWindow("3. Object Detection", cv2.WINDOW_NORMAL)
    cv2.namedWindow("4. Objects + Depth Info", cv2.WINDOW_NORMAL)

    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    sock.bind(("", 5000))
    
    try:
        while True:

            print("-------------------------------")

            packet, addr= sock.recvfrom(65536)
            #print("Re√ßu de :", addr)

            
            size = struct.unpack("H", packet[:2])[0]
            data = packet[2:2+size]

            # D√©codage JPEG
            frame = cv2.imdecode(np.frombuffer(data, dtype=np.uint8), cv2.IMREAD_COLOR)
           
           
            current_time = time.time() 


            # ret, frame = cap.read()
            # if not ret:
            #     break
            
            # ===== FEN√äTRE 1 : Cam√©ra RGB Brute =====
            frame_rgb = frame.copy()
            cv2.imshow("1. Camera RGB", frame_rgb)
            
            # ===== FEN√äTRE 2 : Depth Estimation =====
            with torch.no_grad():
                depth = depth_model.infer_image(frame)
            depth_colored = visualize_depth(depth)
            if depth_colored.shape[:2] != frame.shape[:2]:
                depth_colored = cv2.resize(depth_colored, (frame.shape[1], frame.shape[0]))
            cv2.imshow("2. Depth Estimation", depth_colored)
            
            # ===== FEN√äTRE 3 : Object Detection avec Masques =====
            results = yolo_model(frame, conf=0.5, iou=0.45, verbose=False,classes=OBJECT_CLASSES)
            
            # Collecter les donn√©es des objets
            objects_data = []
            
            for result in results:
                boxes = result.boxes
                masks = result.masks
                
                if masks is None:
                    continue
                
                for i, box in enumerate(boxes):
                    cls = int(box.cls[0])
                    if cls not in OBJECT_CLASSES:
                        continue
                    
                    # R√©cup√©rer le masque
                    mask = masks.data[i].cpu().numpy()
                    
                    # Calculer la profondeur moyenne du masque
                    avg_depth = calculate_mask_depth(mask, depth)
                    
                    if avg_depth is None:
                        continue
                    
                    # Centre de la bounding box
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    center_x = (x1 + x2) / 2
                    center_y = (y1 + y2) / 2
                    
                    objects_data.append({
                        'box': (x1, y1, x2, y2),
                        'mask': mask,
                        'depth': avg_depth,
                        'intensity': 0,  # Sera calcul√© plus tard
                        'class': cls,
                        'center': (center_x, center_y)
                    })
            
            # Traiter les d√©tections avec profondeur (calcule les intensit√©s)
            process_detection_with_depth(objects_data, frame_width, frame_height,tracker,current_time)
            
            # Cr√©er frame annot√©e
            annotated_frame = results[0].plot()
            annotated_frame = draw_roi(annotated_frame, frame_width, frame_height)
            
            # Ligne de s√©paration
            cv2.line(annotated_frame, 
                    (frame_width // 2, 0), 
                    (frame_width // 2, frame_height), 
                    (255, 0, 0), 2)
            
            cv2.imshow("3. Object Detection", annotated_frame)
            
            # ===== FEN√äTRE 4 : Info Objets + Profondeur =====
            info_frame = frame.copy()
            
            for obj in objects_data:
                x1, y1, x2, y2 = obj['box']
                class_name = COCO_NAMES.get(obj['class'], f"class_{obj['class']}")
                depth_percent = obj['depth'] * 100
                
                # Dessiner bounding box
                color = (0, 255, 0) if obj['center'][0] < frame_width / 2 else (255, 0, 0)
                cv2.rectangle(info_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
                
                # Afficher infos
                label = f"{class_name}"
                depth_label = f"D:{depth_percent:.1f}% I:{obj['intensity']}%"
                
                cv2.putText(info_frame, label, (int(x1), int(y1)-25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                cv2.putText(info_frame, depth_label, (int(x1), int(y1)-5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            
            # Ligne de s√©paration
            cv2.line(info_frame, (frame_width // 2, 0), (frame_width // 2, frame_height), (255, 255, 255), 2)
            
            cv2.imshow("4. Objects + Depth Info", info_frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    
    except KeyboardInterrupt:
        print("\n‚õî Arr√™t du programme...")
    
    finally:
        send_command("STOP")
        cap.release()
        cv2.destroyAllWindows()
        if arduino and arduino.is_open:
            arduino.close()
        print("‚úì Nettoyage termin√©")

if __name__ == "__main__":
    if init_arduino():
    	main()
    else:
        print("‚úó Impossible de d√©marrer sans Arduino")
